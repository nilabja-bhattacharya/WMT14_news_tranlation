{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "nmt",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilabja-bhattacharya/WMT14_news_tranlation/blob/master/nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ojDDbtJjNiZ7",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4GIRSWu4KFD",
        "colab_type": "code",
        "outputId": "206feb5c-bc14-46d5-fe83-ce98262a3ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31DaRQ7f4KDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/gdrive/My Drive/WMT14_news_tranlation/nmt model/model/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bWpi44-3NlAW",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as Data\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "plt.switch_backend('agg')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oDfLK1k51ESD"
      },
      "source": [
        "### Language Model from input and output sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dj8fWiH8Nscr",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y_zAZ3kwOOGS",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "#     s = re.sub(r\"([.!?|])\", r\" \\1\", s)\n",
        "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wf6Zs1z-OW7a",
        "colab": {}
      },
      "source": [
        "  def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "#     lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "#         read().strip().split('\\n')\n",
        "    lines1 = open(\"/gdrive/My Drive/corpus/training/hindien-train.tok.\"+lang1).readlines()\n",
        "    lines2 = open(\"/gdrive/My Drive/corpus/training/hindien-train.tok.\"+lang2).readlines()\n",
        "    #print(lines)\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(x), normalizeString(y)] for (x,y) in zip(lines1, lines2)]\n",
        "#     pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "#     print(pairs[:50])\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gnbVCX6wOl-k",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 25\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "keSFnHbcOtEe",
        "outputId": "59328d45-956c-41f8-c65a-258d2e1da588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    #print(input_lang, output_lang, pairs)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('en', 'hi', True)\n",
        "#print(input_lang, output_lang, pairs)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 213492 sentence pairs\n",
            "Trimmed to 153189 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "hi 50584\n",
            "en 51881\n",
            "['कोई अलार  म सट नही ह', 'no alarm set']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CJtCvCGB1PV9"
      },
      "source": [
        "## Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsEU0trFO48N",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        #changes\n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, num_layers, bidirectional=True)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, (hidden, cell) = self.gru(output, (hidden, cell))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KG1R-knn1ULZ"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x8vU1ZOgO_sa",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        #changes\n",
        "        self.gru = nn.LSTM(hidden_size, hidden_size, num_layers, bidirectional=True)\n",
        "        self.out = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell) = self.gru(output, (hidden, cell))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden, cell\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZepkJ0xM1Xee"
      },
      "source": [
        "## Attention Based Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kgYmKqf-Pyfo",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, num_layers,  dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size*2)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size *4 , self.hidden_size)\n",
        "        self.attn_general = nn.Linear(self.max_length, self.max_length)\n",
        "        self.attn_concat = nn.Linear(self.hidden_size*3, self.max_length)\n",
        "        self.lcl_wa_into_hs = nn.Linear(self.hidden_size*2,self.hidden_size*2)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers, bidirectional=True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        #print(embedded.size(), hidden.size())\n",
        "        # attn_weights = F.softmax(\n",
        "        #     self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        X = None\n",
        "        #print(embedded.shape, encoder_outputs.shape)\n",
        "        if attention_type == 'dot':\n",
        "            X = torch.matmul(embedded[0], encoder_outputs.T) #dot product\n",
        "        elif attention_type == 'general':\n",
        "            X = self.attn_general(torch.matmul(self.lcl_wa_into_hs(embedded[0]), encoder_outputs.T)) ## general\n",
        "        elif attention_type == 'concat':\n",
        "            X = self.attn_concat(torch.cat((embedded[0], hidden[0]), 1))  #concat\n",
        "        attn_weights = F.softmax(X, dim = 1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "        #print(attn_applied.shape)\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell) = self.gru(output, (hidden, cell))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, cell, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vq1uvob8PLtt",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index.keys() else UNK_token for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    #print(indexes)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iW46_tp31nl2"
      },
      "source": [
        "## Training of Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXxIlaB_PROS",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        #print(ei)\n",
        "        encoder_output, encoder_hidden, encoder_cell = encoder(\n",
        "            input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            if attention == True:\n",
        "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            if attention == True:\n",
        "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden, decoder_cell = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nECeeiuIPZTR",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JScvs-pc1cf5"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uCuRrbyFPd7k",
        "colab": {}
      },
      "source": [
        " def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    # lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate,  betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "    if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_concat.\"+str(itr)+\".pth\")\n",
        "      encoder.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      encoder_optimizer.load_state_dict(checkpoint_encoder['optimizer_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_concat.\"+str(itr)+\".pth\")\n",
        "      decoder.load_state_dict(checkpoint_decoder['model_state_dict'])\n",
        "      decoder_optimizer.load_state_dict(checkpoint_decoder['optimizer_state_dict'])\n",
        "      epoch = checkpoint_encoder['epoch']\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(itr+1, itr + n_iters + 1):\n",
        "        print(\"Epoch:{0}\".format(iter))\n",
        "        training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(len(pairs))]\n",
        "        for b in range(1,len(pairs)+1):\n",
        "              training_pair = training_pairs[b-1]\n",
        "              input_tensor = training_pair[0]\n",
        "              target_tensor = training_pair[1]\n",
        "\n",
        "              loss = train(input_tensor, target_tensor, encoder,\n",
        "                          decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "              print_loss_total += loss\n",
        "              plot_loss_total += loss\n",
        "        \n",
        "\n",
        "        print_loss_avg = print_loss_total / len(pairs)\n",
        "        print_loss_total = 0\n",
        "        plot_losses.append(print_loss_avg)\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                      iter, iter / n_iters * 100, print_loss_avg))\n",
        "        torch.save({\n",
        "        'epoch': iter,\n",
        "        'model_state_dict': encoder.state_dict(),\n",
        "        'optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "        }, path+\"checkpoint.encoder_checkpoint.attn_concat10.\"+str(iter)+\".pth\")\n",
        "        torch.save({\n",
        "        'epoch': iter,\n",
        "        'model_state_dict': decoder.state_dict(),\n",
        "        'optimizer_state_dict': decoder_optimizer.state_dict(),\n",
        "        }, path+\"checkpoint.decoder_checkpoint.attn_concat10.\"+str(iter)+\".pth\")\n",
        "\n",
        "    #showPlot(plot_losses)\n",
        "    return plot_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v10w8LPPiKW",
        "colab": {}
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zw3GDYEA1jD3"
      },
      "source": [
        "## Evaluation of test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umuYnZ5MPlOT",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initHidden()\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden, encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden, encoder_cell)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            if attention == True:\n",
        "                decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "                decoder_attentions[di] = decoder_attention.data\n",
        "            else:\n",
        "                decoder_output, decoder_hidden, decocer_cell = decoder(\n",
        "                    decoder_input, decoder_hidden, decoder_cell)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == UNK_token:\n",
        "                decoded_words.append('<UNK>')\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "slbV8GrDPrQX",
        "colab": {}
      },
      "source": [
        "def calculate_bleu(pred_trg, real_trg):   \n",
        "    smoothie = SmoothingFunction().method7\n",
        "    return sentence_bleu(real_trg, pred_trg, smoothing_function=smoothie)\n",
        "    # smoothie = SmoothingFunction().method4\n",
        "    # return bleu(real_trg, pred_trg, smoothing_function=smoothie)\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "        #print(calculate_bleu(pair[1], output_words[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "amjpshoaQQX9",
        "colab": {}
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy()[:20][:20], cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(2))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(2))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def evaluateAndShowAttention(encoder, decoder, input_sentence, real_output_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder, decoder, input_sentence)\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    # print('input =', input_sentence)\n",
        "    # print('output =', ' '.join(output_words))\n",
        "    # print('>', input_sentence)\n",
        "    # print('=', real_output_sentence)\n",
        "    # print('<', output_sentence)\n",
        "    # print('')\n",
        "    blu = calculate_bleu(real_output_sentence, output_words[:-1])\n",
        "    # if attention == True:\n",
        "    #   showAttention(input_sentence, output_words, attentions)\n",
        "    return blu\n",
        "\n",
        "\n",
        "def evaluate_on_test(encoder, decoder):\n",
        "  trg = open(\"/gdrive/My Drive/corpus/hindien-dev.tok.hi\").readlines()\n",
        "  real_out = open(\"/gdrive/My Drive/corpus/hindien-dev.tok.en\").readlines()\n",
        "  blu = 0\n",
        "  cnt = 0\n",
        "  for t, r in zip(trg, real_out):\n",
        "      t = t.replace('\\n','')\n",
        "      r = r.replace('\\n', '')\n",
        "      try:\n",
        "        if len(t.split(' ')) < MAX_LENGTH and len(r.split(' ')) < MAX_LENGTH:\n",
        "            blu += evaluateAndShowAttention(encoder, decoder, t, r)\n",
        "            cnt+=1\n",
        "            if cnt == 2500:\n",
        "              break\n",
        "      except:\n",
        "        pass\n",
        "  print(cnt)\n",
        "  return blu/cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rzn--V6h1sjx"
      },
      "source": [
        "## Implementation:\n",
        "\n",
        "Here I'm implemention a sequence to sequence encoder decoder model of bi-LSTM with 256 hidden units using with and without attention and using various types of attention.\n",
        "\n",
        "Encoder converts the input sequences to a normalized vector space and then decoder unit interpret the output sentence from this normalized vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bnhxFgbjTWp",
        "colab_type": "text"
      },
      "source": [
        "## Sequence to sequence model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COTm6E9K8IeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from_checkpoint = True\n",
        "save_checkpoint = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "joNU_JppP5Rp",
        "colab": {}
      },
      "source": [
        "hidden_size = 512\n",
        "num_layers = 2\n",
        "attention = False\n",
        "itr = 5\n",
        "torch.cuda.empty_cache() \n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words, num_layers).to(device)\n",
        "# losses = trainIters(encoder1, decoder1, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_oye1MeO7E-",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyLxAwfVqSFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.seq.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.seq.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QqqCqg0iQBnH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "b9f17c3d-dfa8-4869-d8d8-84b2377fd447"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)\n",
        "# print(blu)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> कटलन डस  कटॉप\n",
            "= catalan desktop\n",
            "< catalan desktop machine desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop desktop\n",
            "\n",
            "> लकिन य 400 साल परानी नाल , जिसम पानी बहता ह ,\n",
            "= but this 400 year old canal , which draws water ,\n",
            "< but it is this is this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this year this is this\n",
            "\n",
            "> कार  नगी हाल म भी उनका स  वागत समारोह आयोजित हआ जहा उन  होन शिक  षा पर व  याख  यान दिया.उनक प  रससक रथसट डनिस न उनक विद  यालय क लिए कोष इकट  ठा करन क लए नत  य क कार  यक  रम प  रस  तत किए , हालाकि उनका दश खद ही आर  थिक-दर  दशा की भयकर चपट म था.कवि न प  राप  त सारी राशि न  ययार  क म बरोजगारो की भलाई क लिए द दी .\n",
            "= a reception was also organised for him at the carnegie hall where he spoke on education , ruth st.ddenis who admired him gave dance-recitals to raise funds for his school , but as her own country had been recently hard hit by an economic crisis , the poet handed over the proceeds for the benefit of the unemployed in new york .\n",
            "< in lectures regarding the training for a public student in the in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in in\n",
            "\n",
            "> इस आडम   बर की दनिया म रहना लगातार मशिकल होता जा रहा था .\n",
            "= it became more and more difficult to live in a world of appearances .\n",
            "< the world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world world\n",
            "\n",
            "> भारतीय परातात  विक सर  वक  षण वर  णन\n",
            "= indian archeological survey description\n",
            "< indian eryllium best of virgin years national archaeological survey karlfeldt archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological archaeological\n",
            "\n",
            "> इस सिद  धात की प  रामाणिकता , एक काल क सस   पष   ट जीन उत   पादो की पहचान पर निर  भर करती ह जो अगली अवस   था को आरभ करत ह .\n",
            "= the validity of this theory lies in the identification of the precise gene products of one phase which triggers the next phase .\n",
            "< the the the a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "> क  रिकट क शौकीन प  रशसक अक  सर किसी बड  ी जीत क बाद पटाख फोड  कर खल क लिए अपन जनन का प  रदर  शन करत ह ।\n",
            "= avid fans of cricket frequently display their fervor for the game by bursting fire-crackers after any major victory .\n",
            "< the game the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            "\n",
            "> ऑस  ट  रलिया / विक  टोरिया\n",
            "= australia / victoria\n",
            "< australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia / australia /\n",
            "\n",
            "> टगो ( नत  य )\n",
            "= tango\n",
            "< tango nahi ( highness ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (\n",
            "\n",
            "> हा , अब उन  नत कर\n",
            "= yes , upgrade now\n",
            "< yes , yes , yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes yes\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVtF83349ir-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIWsapmMyjvC",
        "colab_type": "text"
      },
      "source": [
        "## Attention Based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBG5ISwYCx_M",
        "colab_type": "text"
      },
      "source": [
        "### DOT variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_fCEhU1xJD",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "attention = True\n",
        "itr = 10\n",
        "attention_type = 'dot'\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers, dropout_p=0.1).to(device)\n",
        "#losses = trainIters(encoder1, decoder1, 2, print_every=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTbOLybXnY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCS9p7Ffu-7r",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_dot.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_dot.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaSkCJ01xJJ",
        "colab_type": "code",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "2bfe4228-120f-4a72-abdb-f2cf20e42aaa"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> ( बी ) प  रशिक  षण और कार  य अनभव\n",
            "= -lrb- b -rrb- training and work experience\n",
            "< ( applause -rrb- training and training training <EOS>\n",
            "\n",
            "> और म अरब-इजरायल सघर  ष क सम  बन  ध म हमास की विजय को लकर तटस  थ ह .\n",
            "= not much separates hamas anti-zionism from fatah anti-zionism except that hamas terrorists speak forthrightly while fatah terrorists obfuscate . even their tactics overlap , as fatah denies the existence of israel and hamas negotiates with israelis . differing emphases and styles , more than substance , distinguishes their attitudes toward israel .\n",
            "< and i hamas hamas hamas anti-zionism anti-zionism the hamas the hamas the . . . . <EOS>\n",
            "\n",
            "> सप  टिसीमिया क  या ह ?\n",
            "= what is septicaemia ?\n",
            "< what is the ? ? <EOS>\n",
            "\n",
            "> स  वर\n",
            "= vowel\n",
            "< swara <EOS>\n",
            "\n",
            "> पासवर  ड रख ( ओई सफ )\n",
            "= set password ( oi safe )\n",
            "< encrypt password ( oi safe ) <EOS>\n",
            "\n",
            "> पर  चो म मसलमानो को नौकरी पर रखन और उनस खरीद-फरोत करन क खिलफ चतावनी दी गई ह .\n",
            "= they warn against employing muslims , buying from them or selling to them .\n",
            "< the the often to to and and to to to to to to . . <EOS>\n",
            "\n",
            "> कब उसकी पलक धीर - धीर हिलन लगी , उस पता नही चला ।\n",
            "= he did not even notice her eyelashes begin to quiver .\n",
            "< the had had to to , , to . . . <EOS>\n",
            "\n",
            "> आदिपर  व\n",
            "= adi parva\n",
            "< adi parva <EOS>\n",
            "\n",
            "> महाकवि ग.द. माडगळकर एव सधीर फड  क रचित मराठी गीतरामायण\n",
            "= great poet gd madgulkar and sudheer phadke has written marathi geet ramayana\n",
            "< great poet and sudhir madgunkar madgunkar sudhir sudhir <EOS>\n",
            "\n",
            "> यआरआई नही प  रदान किया गया\n",
            "= no uri provided\n",
            "< no uri not not loaded <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXjD_qmy1xJU",
        "colab_type": "code",
        "trusted": true,
        "colab": {}
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsle1xXRC14P",
        "colab_type": "text"
      },
      "source": [
        "### General Variant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jphas_dDoCVt",
        "colab_type": "text"
      },
      "source": [
        "#### Bi-LSTM with 256 unit and 2 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "-tjaR0YyZaj-",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "attention = True\n",
        "itr = 9\n",
        "attention_type = 'general'\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers, dropout_p=0.1).to(device)\n",
        "#losses = trainIters(encoder1, decoder1, 100000, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2XKyzEZ0zotZ",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_general.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_general.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDy0M-h4Xdm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "BBN1Nf0hZakD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "3d5467b1-8098-4ccc-ee00-695a75300c07"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> उस गति स सबधित मीट  रिक जिनस % { short _ product _ name } अनरोधित कार  यवाही निष  पादित करता ह\n",
            "= metrics relating to the speed with which % { short _ product _ name } performs requested actions\n",
            "< metrics relating to add % { short _ frame _ frame } from the requested _ frame <EOS>\n",
            "\n",
            "> उनकी निष   ठा अपन घरो म भ  ट  टियो म आग जलान , अपन दवता क लिए भजन गान और चावल , दध सोमरस या पश बलि क रप म अर  पित करन म थी .\n",
            "= their devotion consisted in burning fires in their hearths , singing hymns to their gods and offering rice , milk , soma or animals as sacrifice .\n",
            "< his speeches in his his , his his his , his , his , his , his , , and and and and and his , <EOS>\n",
            "\n",
            "> बाबर और उसक पत  र हमाय दोनो क पास भारत म राजनतिक और सास   कतिक एकता उत   पन   न करन का महान कार  य करन की व   यापक दष   टि तथा कल   पना थी , कित उनका शासन बहत कम समय तक रहा .\n",
            "= both babur and his son humayun had the breadth of vision and the imagination to set about the great task of creating political and cultural unity in india ; but they had very brief reigns .\n",
            "< the the and the and and and and and and the the the the the the the the the the the , and , , , , and and and the . <EOS>\n",
            "\n",
            "> अगर आपको अपनी स  थिति क बार म कोई शइका हो , तो हस  ताक  षर न कीजिए और घर वापस लौटन तक इतजार कीजिए और उसक बाद काननी सलाह लीजिए ।\n",
            "= if you are uncertain about your position you should not sign but wait until you get home and seek legal advice .\n",
            "< if you have the , , your , , , , your , , , , , you and your <EOS>\n",
            "\n",
            "> उनम स एक म चावल क अतिरिक  त सब कछ हो ; दसरी म दही क अलावा सब कछ हो .\n",
            "= one of them has all the items except rice , the other has all the items except curds .\n",
            "< some of them have to to few of , , to of . . . . . . . <EOS>\n",
            "\n",
            "> और मझ करीब २०० तरह क दश पता ह , और मझ आकड पता ह ।\n",
            "= and i know 200 , i know about the small data .\n",
            "< and i have a small figure of my data , i know . <EOS>\n",
            "\n",
            "> शीट\n",
            "= sheets\n",
            "< sheet <EOS>\n",
            "\n",
            "> जब प  यार किसी स होता ह\n",
            "= jab pyar kisi se hota hai\n",
            "< jab pyaar kisi <EOS>\n",
            "\n",
            "> खजान\n",
            "= repositories ...\n",
            "< repositories <EOS>\n",
            "\n",
            "> पालि भाषा का साहित  य\n",
            "= pali literature\n",
            "< literature literature <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "mLS8LWr3ZakH",
        "colab": {}
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S55fO5apoQR9",
        "colab_type": "text"
      },
      "source": [
        "#### Bi-LSTM with 512 unit and 4 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "MC8Cdg5JoXKf",
        "colab": {}
      },
      "source": [
        "hidden_size = 512\n",
        "num_layers = 4\n",
        "attention = True\n",
        "itr = 6\n",
        "attention_type = 'general'\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers, dropout_p=0.1).to(device)\n",
        "#losses = trainIters(encoder1, decoder1, 100000, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YanK_TdZoXKl",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_general512.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_general512.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r3AccD6HoXKp",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "5Fn9az6noXKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "a2f83cc7-2b72-4b83-efba-10d2361a3bf6"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> व टस  ट क  रिकट म सबस ज   यादा रन बनान वाल बल  लबाज  ह ।\n",
            "= he has scored the maximum runs in test cricket .\n",
            "< they is the the of the the cricket of . . <EOS>\n",
            "\n",
            "> pulse aqui para cambiar su apodo\n",
            "= click to change your nickname\n",
            "< click to change the nickname <EOS>\n",
            "\n",
            "> लवासा\n",
            "= lavasa\n",
            "< shirdi <EOS>\n",
            "\n",
            "> changer de mot de passe ...\n",
            "= change password ...\n",
            "< change nickname ... <EOS>\n",
            "\n",
            "> जाचसची\n",
            "= checklist\n",
            "< polish <EOS>\n",
            "\n",
            "> म अपन प  रदाता नही ढढ  सकता ह और म इस दस  ती रप स दाखिल करना चाहता ह ( m ) :\n",
            "= i can &apos;t find my provider and i wish to enter it manually :\n",
            "< i can want to want to my my i i i i : <EOS>\n",
            "\n",
            "> यह सब फ  रिश  त नही ह ,\n",
            "= all of them are not angels ,\n",
            "< the all is not not , <EOS>\n",
            "\n",
            "> सशन कार  यक  रम\n",
            "= session programs\n",
            "< marquee survey <EOS>\n",
            "\n",
            "> आप क  या कर सकत ह अगर आप को शिकायत करनी हो ।\n",
            "= what to do if you wish to complain\n",
            "< what if if if if if if you can . <EOS>\n",
            "\n",
            "> सिर  फ बड अक  षरो का ही साराश लिख\n",
            "= show uppercase text only\n",
            "< only the the the the <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "outputId": "0595ef02-068c-4845-ce0f-31fa6c7d7fd0",
        "id": "7KpDZ0VyoXKw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500\n",
            "0.23886061729751382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ozReOhvOz3",
        "colab_type": "text"
      },
      "source": [
        "#### Sentence length 25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubmXoplbvUb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 512\n",
        "num_layers = 4\n",
        "attention = True\n",
        "itr = 12\n",
        "attention_type = 'general'\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers, dropout_p=0.1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "stclUdtZvSfV",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_general51225.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_general51225.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "myK2NafrvSfc",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "2si28K9qvSfg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "865e0349-b5df-475d-ba4b-2705f68d4e48"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> एवोल  यशन बकअप जाच\n",
            "= check evolution back up\n",
            "< check evolution address <EOS>\n",
            "\n",
            "> डिवाइस चयन\n",
            "= device selection\n",
            "< the device <EOS>\n",
            "\n",
            "> नोबल परस  कार जीतन क बाद , वह चार दशको तक गमनामी म जीवन व  यतीत करता रहा ।\n",
            "= after winning the noble prize , he lived in obscurity for four decades .\n",
            "< after the father of his noble of the the he the father of his father . <EOS>\n",
            "\n",
            "> घ  l18 आप को सामाजिक निधी स मदद\n",
            "= gl18 help from the social fund\n",
            "< gl18 you tenant you benefit <EOS>\n",
            "\n",
            "> इडफिनिट\n",
            "= indefinite\n",
            "< rwanda <EOS>\n",
            "\n",
            "> अल  लाह उस गमराह कर दगा ।\n",
            "= allah will destroy them\n",
            "< allah will make it misleading . <EOS>\n",
            "\n",
            "> उन  होन पहल ही हम चतावनी दी थी । &lt; s &gt; यह शरआत ह ।\n",
            "= they told us what to expect . now it ’ s beginning .\n",
            "< they asked that we lesson . <EOS>\n",
            "\n",
            "> तो हम एक दसरी धारणा की और कदम बढ  ात ह ,\n",
            "= then we move on to a second assumption ,\n",
            "< so we have a a a a a , , <EOS>\n",
            "\n",
            "> क  यकि और कही ,\n",
            "= because around the corner ,\n",
            "< because that &apos;s , <EOS>\n",
            "\n",
            "> कार  यक  रम का नाम बदल ( n ) ...\n",
            "= rename event ...\n",
            "< rename event ... <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "outputId": "c4c7c029-e109-443f-d1cf-994b676faab9",
        "id": "-k58yIt6vSfl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2500\n",
            "0.2752689768443108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1e5Vsf4C5DD",
        "colab_type": "text"
      },
      "source": [
        "### Concat Varient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "L_u9kt18ZfGh",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "attention_type = 'concat'\n",
        "attention = True\n",
        "itr = 5\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, num_layers).to(device)\n",
        "decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, num_layers, dropout_p=0.1).to(device)\n",
        "#losses = trainIters(encoder1, decoder1, 5, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bCheRC8PyaOF",
        "colab": {}
      },
      "source": [
        "if from_checkpoint == True:\n",
        "      checkpoint_encoder = torch.load(path+\"checkpoint.encoder_checkpoint.attn_concat.\"+str(itr)+\".pth\")\n",
        "      encoder1.load_state_dict(checkpoint_encoder['model_state_dict'])\n",
        "      checkpoint_decoder = torch.load(path+\"checkpoint.decoder_checkpoint.attn_concat.\"+str(itr)+\".pth\")\n",
        "      decoder1.load_state_dict(checkpoint_decoder['model_state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9H-gYGfXkhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  showPlot(losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dda8x8VyYutP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.save(losses, 'losses_concat.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "ffCRjsCHZfGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "404e6b12-410f-4d13-a575-d16143a0b066"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> पिछल हत जब एअर इडिया का विशष विमान कष  णदव राय , जिस पर 16वी सदी क सम  राट का नाम लिखा हा था , प  रधानमत  री वाजपयी को विएतनाम और इडोनशिया ल गया तो भारत की नई पर  वाभिमख कटनीति म नया मोड आ गया .\n",
            "= last week , when air-india &apos; s special aircraft , krishna deva raya , flew to vietnam and indonesia with prime minister atal bihari vajpayee , the name of the 16th century king painted on it added an ironic twist to india &apos; s new look-east diplomacy .\n",
            "< in punjab , the punjab of punjab , punjab , punjab , punjab , punjab , punjab , punjab , punjab , punjab , punjab , punjab , the punjab of the punjab . the punjab of the punjab . <EOS>\n",
            "\n",
            "> जिस जररत ह उसक लिय , एन एच एस व  दारा हियरिग एडज , ( श  रवण सहायक उपकरण ) बटरिया और उनकी मरम  मत मफ  त ह ।\n",
            "= for those who need them , hearing aids batteries or repairs are free on the nhs .\n",
            "< for example , for example , solvents , , , , , , , , , , , , , <EOS>\n",
            "\n",
            "> श  री कष  ण न गीता का सन  दश अर  जन को सनाया था ।\n",
            "= shree krishna told arjun about preachings of gita\n",
            "< sri krishna krishna krishna was killed by shri krishna <EOS>\n",
            "\n",
            "> पाठ साफ  कर\n",
            "= clear text\n",
            "< clear text <EOS>\n",
            "\n",
            "> नजीब महफज \n",
            "= naguib mahfouz\n",
            "< sonakshi <EOS>\n",
            "\n",
            "> चीन की लिखित भाषा प  रणाली विश  व की सबस परानी ह जो आज तक उपयोग म लायी जा रही ह और जो कई आविष  कारो का स  रोत भी ह ।\n",
            "= the chines written language is the oldest in the universe and is still being used has been used in innovative ways\n",
            "< the language of the is the is the is the language is now is the language and the language <EOS>\n",
            "\n",
            "> खोल\n",
            "= open\n",
            "< open <EOS>\n",
            "\n",
            "> टगो ( नत  य )\n",
            "= tango\n",
            "< tango ( moon ) <EOS>\n",
            "\n",
            "> मबई भारतीय चलचित  र का जन  मस  थान ह ।\n",
            "= mumbai is the birthplace of the indian movie industry .\n",
            "< mumbai indian history of indian history of mumbai <EOS>\n",
            "\n",
            "> आपको और सीढिया मिल जाती ह ।\n",
            "= you will find new stairs .\n",
            "< you will be able to and <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "trusted": true,
        "id": "bo87Dc3aZfGq",
        "colab": {}
      },
      "source": [
        "blu = evaluate_on_test(encoder1, decoder1)\n",
        "print(blu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niAbu3E0y07s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}